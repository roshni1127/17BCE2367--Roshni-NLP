{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133737"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2 CMU wordlist\n",
    "import nltk \n",
    "entries = nltk.corpus.cmudict.entries()\n",
    "len(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n"
     ]
    }
   ],
   "source": [
    "for entry in entries[10000:10025]:\n",
    "    print(entries)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n"
     ]
    }
   ],
   "source": [
    "for entry in entries[10000:10025]:\n",
    "    print(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n",
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n"
     ]
    }
   ],
   "source": [
    "for entry in entries[10000:10025]:\n",
    "    print(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-76052bf014ba>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-76052bf014ba>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    form nltk.corpus import wordnet as wn\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#3. wordnet \n",
    "form nltk.corpus import wordnet as wn\n",
    "#wn.synsets('motorcar') #we get id for subset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-f3c18fcb45df>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-f3c18fcb45df>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    form nltk.corpus import wordnet as wn\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "form nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('motorcar') #we get id for subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'auto', 'automobile', 'machine', 'motorcar']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-14-79cf690f756b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-14-79cf690f756b>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    texts=[\"\"\"Benedict Timothy Carlton Cumberbatch CBE (born 19 July 1976) is an English actor. A graduate of the Victoria University of Manchester, he continued his training at the London Academy of Music and Dramatic Art, obtaining a Master of Arts in Classical Acting. He first performed at the Open Air Theatre, Regent's Park in Shakespearean productions and made his West End debut in Richard Eyre's revival of Hedda Gabler in 2005. \"\"\"\"]\u001b[0m\n\u001b[1;37m                                                                                                                                                                                                                                                                                                                                                                                                                                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "texts=[\"\"\"Benedict Timothy Carlton Cumberbatch CBE (born 19 July 1976) is an English actor. A graduate of the Victoria University of Manchester, he continued his training at the London Academy of Music and Dramatic Art, obtaining a Master of Arts in Classical Acting. He first performed at the Open Air Theatre, Regent's Park in Shakespearean productions and made his West End debut in Richard Eyre's revival of Hedda Gabler in 2005. \"\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-15-cd66f45ef397>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-cd66f45ef397>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    texts=[\"\"\"Benedict Timothy Carlton Cumberbatch CBE (born 19 July 1976) is an English actor. A graduate of the Victoria University of Manchester, he continued his training at the London Academy of Music and Dramatic Art, obtaining a Master of Arts in Classical Acting. He first performed at the Open Air Theatre, Regent's Park in Shakespearean productions and made his West End debut in Richard Eyre's revival of Hedda Gabler in 2005. \"\"\"\"]\u001b[0m\n\u001b[1;37m                                                                                                                                                                                                                                                                                                                                                                                                                                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "texts=['''Benedict Timothy Carlton Cumberbatch CBE (born 19 July 1976) is an English actor. A graduate of the Victoria University of Manchester, he continued his training at the London Academy of Music and Dramatic Art, obtaining a Master of Arts in Classical Acting. He first performed at the Open Air Theatre, Regent's Park in Shakespearean productions and made his West End debut in Richard Eyre's revival of Hedda Gabler in 2005. ''']\n",
    "for text in texts:\n",
    "       sentences = nltk.sent_tokenize(text)\n",
    "       for sentence in sentences:\n",
    "           words = nltk.word_tokenize(sentence)\n",
    "           tagged_words = nltk.pos_tag(words)\n",
    "           print(tagged_words)\n",
    "           \n",
    "                   \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Benedict', 'NNP'), ('Timothy', 'NNP'), ('Carlton', 'NNP'), ('Cumberbatch', 'NNP'), ('CBE', 'NNP'), ('(', '('), ('born', 'JJ'), ('19', 'CD'), ('July', 'NNP'), ('1976', 'CD'), (')', ')'), ('is', 'VBZ'), ('an', 'DT'), ('English', 'JJ'), ('actor', 'NN'), ('.', '.')]\n",
      "[('A', 'DT'), ('graduate', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Victoria', 'NNP'), ('University', 'NNP'), ('of', 'IN'), ('Manchester', 'NNP'), (',', ','), ('he', 'PRP'), ('continued', 'VBD'), ('his', 'PRP$'), ('training', 'NN'), ('at', 'IN'), ('the', 'DT'), ('London', 'NNP'), ('Academy', 'NNP'), ('of', 'IN'), ('Music', 'NNP'), ('and', 'CC'), ('Dramatic', 'NNP'), ('Art', 'NNP'), (',', ','), ('obtaining', 'VBG'), ('a', 'DT'), ('Master', 'NN'), ('of', 'IN'), ('Arts', 'NNS'), ('in', 'IN'), ('Classical', 'JJ'), ('Acting', 'NNP'), ('.', '.')]\n",
      "[('He', 'PRP'), ('first', 'RB'), ('performed', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('Open', 'NNP'), ('Air', 'NNP'), ('Theatre', 'NNP'), (',', ','), ('Regent', 'NNP'), (\"'s\", 'POS'), ('Park', 'NNP'), ('in', 'IN'), ('Shakespearean', 'NNP'), ('productions', 'NNS'), ('and', 'CC'), ('made', 'VBD'), ('his', 'PRP$'), ('West', 'JJ'), ('End', 'NN'), ('debut', 'NN'), ('in', 'IN'), ('Richard', 'NNP'), ('Eyre', 'NNP'), (\"'s\", 'POS'), ('revival', 'NN'), ('of', 'IN'), ('Hedda', 'NNP'), ('Gabler', 'NNP'), ('in', 'IN'), ('2005', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "texts=['''Benedict Timothy Carlton Cumberbatch CBE (born 19 July 1976) is an English actor. A graduate of the Victoria University of Manchester, he continued his training at the London Academy of Music and Dramatic Art, obtaining a Master of Arts in Classical Acting. He first performed at the Open Air Theatre, Regent's Park in Shakespearean productions and made his West End debut in Richard Eyre's revival of Hedda Gabler in 2005. ''']\n",
    "for text in texts:\n",
    "       sentences = nltk.sent_tokenize(text)\n",
    "       for sentence in sentences:\n",
    "           words = nltk.word_tokenize(sentence)\n",
    "           tagged_words = nltk.pos_tag(words)\n",
    "           print(tagged_words)\n",
    "           \n",
    "                   \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-a2d74a2196d8>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-18-a2d74a2196d8>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    fomr nltk.tokenize import TweetTokenize\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# twitter aware tokenizer\n",
    "\n",
    "import nltk \n",
    "fomr nltk.tokenize import TweetTokenize\n",
    "text='PRESIDENTIAL HARASSMENT!'\n",
    "twtkn=TweetTokenizer()\n",
    "twtkn.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-a2d74a2196d8>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-19-a2d74a2196d8>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    fomr nltk.tokenize import TweetTokenize\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# twitter aware tokenizer\n",
    "\n",
    "import nltk \n",
    "from nltk.tokenize import TweetTokenize\n",
    "text='PRESIDENTIAL HARASSMENT!'\n",
    "twtkn=TweetTokenizer()\n",
    "twtkn.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TweetTokenize' from 'nltk.tokenize' (C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-43ce764db4c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTweetTokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'PRESIDENTIAL HARASSMENT!'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtwtkn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTweetTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'TweetTokenize' from 'nltk.tokenize' (C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# twitter aware tokenizer\n",
    "\n",
    "import nltk \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "text='After years of rebuilding OTHER nations, we are finally rebuilding OUR nation. We are finally putting AMERICA FIRST! #KAG2020'\n",
    "twtkn=TweetTokenizer()\n",
    "twtkn.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['After',\n",
       " 'years',\n",
       " 'of',\n",
       " 'rebuilding',\n",
       " 'OTHER',\n",
       " 'nations',\n",
       " ',',\n",
       " 'we',\n",
       " 'are',\n",
       " 'finally',\n",
       " 'rebuilding',\n",
       " 'OUR',\n",
       " 'nation',\n",
       " '.',\n",
       " 'We',\n",
       " 'are',\n",
       " 'finally',\n",
       " 'putting',\n",
       " 'AMERICA',\n",
       " 'FIRST',\n",
       " '!',\n",
       " '#KAG2020']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "text='After years of rebuilding OTHER nations, we are finally rebuilding OUR nation. We are finally putting AMERICA FIRST! #KAG2020'\n",
    "twtkn=TweetTokenizer()\n",
    "twtkn.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can: 94 could: 87 may: 93 might: 38 must: 53 will: 389 "
     ]
    }
   ],
   "source": [
    "#frequency distribution BROWN CORPUS\n",
    "\n",
    "from nltk.corpus import brown \n",
    "news_text = brown.words(categories='news')\n",
    "fdist = nltk.FreqDist(w.lower() for w in news_text)\n",
    "modals = ['can', 'could', 'may', 'might','must','will']\n",
    "for m in modals:\n",
    "    print(m+':',fdist[m],end=' ')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
